{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object Detection: AML Package for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Object Detection is one of the main problems in Computer Vision. Traditionally, this required expert knowledge to identify and implement so called “features” that highlight the position of objects in the image. Starting in 2012 with the famous [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), Deep Neural Networks are used to automatically find these features.\n",
    "\n",
    "This notebook shows how the Azure Machine Learning Package for Computer Vision can be used to train, evaluate, and deploy a [Faster R-CNN](https://arxiv.org/abs/1506.01497) object detection model. The **Azure Machine Learning Package for Computer Vision** makes it easy to perform all these steps, and internally uses [Tensorflow's implementation](https://arxiv.org/abs/1611.10012) of the model. This model was shown to produce state-of-the-art results for Pascal VOC, one of the main object detection challenges in the field. For more information, see the [Tensorflow object detection website](https://github.com/tensorflow/models/tree/master/research/object_detection).\n",
    "\n",
    "When building and deploying a model with this package, you go through the following steps:\n",
    "1.\tDataset Creation\n",
    "2.\tDeep Neural Network (DNN) Model Definition\n",
    "3.\tModel Training\n",
    "4.\tEvaluation and Visualization\n",
    "5.\tWeb service Deployment\n",
    "6.\tWeb service Load Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Dataset\n",
    "\n",
    "For this demo, a dataset of grocery items inside refrigerators is provided, consisting of 30 images and 8 classes (eggBox, joghurt, ketchup, mushroom, mustard, orange, squash, and water). For each jpg image, there's an annotation xml-file with similar name. \n",
    "\n",
    "The following figure shows the recommended folder structure. \n",
    "![folder structure](media/how-to-build-deploy-object-detection-models/data_directory.JPG)\n",
    "\n",
    "## Image Annotation\n",
    "\n",
    "Annotated object locations are required to train and evaluate an object detector. [LabelImg](https://tzutalin.github.io/labelImg) is an open source annotation tool that can be used to annotate images. LabelImg writes an xml-file per image in Pascal-VOC format, which can be read by this package. \n",
    "\n",
    "Here's a screenshot of the tool's UI.\n",
    "![tool UI](media/how-to-build-deploy-object-detection-models/labeImg.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Context\n",
    "The storage context is used to determine where various output files such as DNN model files are stored. For more information, see the [StorageContext documentation](https://docs.microsoft.com/en-us/python/api/cvtk.core.context.storagecontext?view=azure-ml-py-latest). Normally, the storage content does not need to be set explicitly. However, to avoid the Workbench project size limit of 25 MB, set the outputs directory to point to a location outside the AML project (\"../../../../cvtk_output\"). Make sure to remove the \"cvtk_output\" directory once it is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os, time\n",
    "from cvtk.core import Context, ObjectDetectionDataset, TFFasterRCNN\n",
    "from cvtk.utils import detection_utils\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Disable printing of logging messages\n",
    "from azuremltkbase.logging import ToolkitLogger\n",
    "ToolkitLogger.getInstance().setEnabled(False)\n",
    "\n",
    "# Initialize the context object\n",
    "out_root_path = \"../../../cvtk_output\"\n",
    "Context.create(outputs_path=out_root_path, persistent_path=out_root_path, temp_path=out_root_path)\n",
    "\n",
    "# Display the images\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "Create a CVTK dataset that consists of a set of images, with their respective bounding box annotations. In this example, the refrigerator images that are provided in the \"../sample_data/foods/training\" folder are used. Only JPEG images are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_folder = \"../sample_data/foods/train\"\n",
    "data_train = ObjectDetectionDataset.create_from_dir(dataset_name='training_dataset', data_dir=image_folder,\n",
    "                                                    annotations_dir=\"Annotations\", image_subdirectory='JPEGImages')\n",
    "\n",
    "# Show some statistics of the training image, and also give one example of the ground truth rectangle annotations\n",
    "data_train.print_info()\n",
    "_ = data_train.images[2].visualize_bounding_boxes(image_size = (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define a model\n",
    "\n",
    "In this example, the Faster R-CNN model is used. Various parameters can be provided when defining this model. The meaning of these parameters, as well as the parameters used for training (see next section) can be found in either CVTK's API docs, or on the [Tensorflow object detection website](https://github.com/tensorflow/models/tree/master/research/object_detection). More information about Faster R-CNN model can be found at [this link](https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Faster-R-CNN#technical-details). This model is based on Fast R-CNN and more information about it can be found [here](https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN#algorithm-details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.0       # Threshold on the detection score, use to discard lower-confidence detections.\n",
    "max_total_detections = 300  # Maximum number of detections. A high value slows down training but might increase accuracy.\n",
    "my_detector = TFFasterRCNN(labels=data_train.labels, \n",
    "                           score_threshold=score_threshold, \n",
    "                           max_total_detections=max_total_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train the model\n",
    "\n",
    "The COCO-trained Faster R-CNN model with ResNet50 is used as the starting point for training. \n",
    "\n",
    "In this example, the number of detector training steps is set to 350 for speedy training (~5 minutes with GPU). However, in practice, a good rule of thumb is to set the steps to 10 or more times the number of images in the training set.\n",
    "\n",
    "Two key parameters for training are:\n",
    "- Number of steps to train the model, represented by the num_seps argument. Each step trains the model with a minibatch of batch size one\n",
    "- Learning rate(s), which can be set by initial_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"tensorboard --logdir={}\".format(my_detector.train_dir))\n",
    "\n",
    "# to get good results, use a larger value for num_steps, e.g., 5000.\n",
    "num_steps = 350\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "start_train = time.time()\n",
    "my_detector.train(dataset=data_train, num_steps=num_steps, \n",
    "                  initial_learning_rate=learning_rate)\n",
    "end_train = time.time()\n",
    "print(end_train-start_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard can be used to visualize the training progress. TensorBoard events are located in the folder specified by the model object's train_dir attribute. To view TensorBoard, follow these steps:\n",
    "1. Copy the printout that starts with 'tensorboard --logdir' to a command line and run it. \n",
    "2. Copy the returned URL from the command line to a web browser to view the TensorBoard. \n",
    "\n",
    "The TensorBoard should look like the following screenshot. It takes a few moments for the training folder to be populated. So if TensorBoard does not show up correctly the first time try repeating steps 1-2. \n",
    "\n",
    "![tensorboard](media/how-to-build-deploy-object-detection-models/tensorboard.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The 'evaluate' method is used to evaluate the model. This function requires an ObjectDetectionDataset object as an input. The evaluation dataset can be created using the same function as the one used for the training dataset. The supported metric is Average Precision as defined for the [PASCAL VOC Challenge](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_folder = \"../sample_data/foods/test\"\n",
    "data_val = ObjectDetectionDataset.create_from_dir(dataset_name='val_dataset', data_dir=image_folder)\n",
    "eval_result = my_detector.evaluate(dataset=data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results can be printed out in a clean format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the performance metric values\n",
    "for label_obj in data_train.labels:\n",
    "    label = label_obj.name\n",
    "    key = 'PASCAL/PerformanceByCategory/AP@0.5IOU/' + label\n",
    "    print('{0: <15}: {1: <3}'.format(label, round(eval_result[key], 2)))\n",
    "print('{0: <15}: {1: <3}'.format(\"overall:\", round(eval_result['PASCAL/Precision/mAP@0.5IOU'], 2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can compute the accuracy of the model on the training set. Doing this helps make sure training converged to a good solution. The accuracy on the training set after successful training is often close to 100%.\n",
    "\n",
    "Evaluation results can also be viewed from TensorBoard, including mAP values and images with predicted bounding boxes. Copy the printout from the following code into a command line window to start the TensorBoard client. Here a port value 8008 is used to avoid conflict with the default value of 6006, which was using for viewing training status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"tensorboard --logdir={} --port=8008\".format(my_detector.eval_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score an image\n",
    "\n",
    "Once you're satisfied with the performance of the trained model, the model object's 'score' function can be used to score new images. The returned scores can be visualized with the 'visualize' function . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = data_val.images[1].storage_path\n",
    "detections_dict = my_detector.score(image_path)\n",
    "path_save = out_root_path + \"/scored_images/scored_image_preloaded.jpg\"\n",
    "ax = detection_utils.visualize(image_path, detections_dict, image_size=(8, 12))\n",
    "path_save_dir = os.path.dirname(os.path.abspath(path_save))\n",
    "os.makedirs(path_save_dir, exist_ok=True)\n",
    "ax.get_figure().savefig(path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Save the model\n",
    "\n",
    "The trained model can be saved to disk, and loaded back into memory, as shown in the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "save_model_path = out_root_path + \"/frozen_model/faster_rcnn.model\" # Please save your model to outside of your AML workbench project folder because of the size limit of AML project\n",
    "my_detector.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved model for scoring\n",
    "\n",
    "To use the saved model, load the model into memory with the 'load' function. You only need to load once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_detector_loaded = TFFasterRCNN.load(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is loaded, it can be used to score an image or a list of images. For a single image, a dictionary is returned with keys such as 'detection_boxes', 'detection_scores', and 'num_detections'. If the input is a list of images, a list of dictionary is returned, with one dictionary corresponding to one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_dict = my_detector_loaded.score(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detected objects with scores above 0.5, including labels, scores, and coordinates can be printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up = dict((v,k) for k,v in my_detector.class_map.items())\n",
    "n_obj = 0\n",
    "for i in range(detections_dict['num_detections']):\n",
    "    if detections_dict['detection_scores'][i] > 0.5:\n",
    "        n_obj += 1\n",
    "        print(\"Object {}: label={:11}, score={:.2f}, location=(top: {:.2f}, left: {:.2f}, bottom: {:.2f}, right: {:.2f})\".format(\n",
    "            i, look_up[detections_dict['detection_classes'][i]], \n",
    "            detections_dict['detection_scores'][i], \n",
    "            detections_dict['detection_boxes'][i][0],\n",
    "            detections_dict['detection_boxes'][i][1], \n",
    "            detections_dict['detection_boxes'][i][2],\n",
    "            detections_dict['detection_boxes'][i][3]))    \n",
    "        \n",
    "print(\"\\nFound {} objects in image {}.\".format(n_obj, image_path))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the scores just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "path_save = out_root_path + \"/scored_images/scored_image_frozen_graph.jpg\"\n",
    "ax = detection_utils.visualize(image_path, detections_dict, path_save=path_save, image_size=(8, 12))\n",
    "# ax.get_figure() # use this code extract the returned image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalization: deploy and consume\n",
    "\n",
    "<b>Prerequisites:</b> \n",
    "Check the **Prerequisites** section of the deployment notebook to set up your deployment CLI. You only need to set it up once for all your deployments. More deployment-related topics including IoT Edge deployment can be found in the deployment notebook.\n",
    "       \n",
    "<b>Deployment API:</b>\n",
    "\n",
    "> **Examples:**\n",
    "- ```deploy_obj = AMLDeployment(deployment_name=deployment_name, associated_DNNModel=dnn_model, aml_env=\"cluster\")``` # create deployment object\n",
    "- ```deploy_obj.deploy()``` # deploy web service\n",
    "- ```deploy_obj.status()``` # get status of deployment\n",
    "- ```deploy_obj.score_image(local_image_path_or_image_url)``` # score an image\n",
    "- ```deploy_obj.delete()``` # delete the web service\n",
    "- ```deploy_obj.build_docker_image()``` # build docker image without creating webservice\n",
    "- ```AMLDeployment.list_deployment()``` # list existing deployment\n",
    "- ```AMLDeployment.delete_if_service_exist(deployment_name)``` # delete if the service exists with the deployment name\n",
    "\n",
    "<b>Deployment management with portal:</b>\n",
    "\n",
    "You can go to [Azure portal](https://ms.portal.azure.com/) to track and manage your deployments. From Azure portal, find your Machine Learning Model Management account page (You can search for your model management account name). Then go to: the model management account page->Model Management->Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### OPTIONAL - Interactive CLI setup helper ###### \n",
    "# # Interactive CLI setup helper, including model management account and deployment environment.\n",
    "# # If you haven't setup you CLI before or if you want to change you CLI settings, you can use this block to help you interactively.\n",
    "# # UNCOMMENT THE FOLLOWING LINES IF YOU HAVE NOT CREATED OR SET THE MODEL MANAGEMENT ACCOUNT AND DEPLOYMENT ENVIRONMENT\n",
    "\n",
    "# from azuremltkbase.deployment import CliSetup\n",
    "# CliSetup().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvtk.operationalization import AMLDeployment\n",
    "\n",
    "# set deployment name\n",
    "deployment_name = \"wsdeployment\"\n",
    "\n",
    "# Create deployment object\n",
    "# It will use the current deployment environment (you can check it with CLI command \"az ml env show\").\n",
    "deploy_obj = AMLDeployment(deployment_name=deployment_name, aml_env=\"cluster\", associated_DNNModel=my_detector, replicas=1)\n",
    "\n",
    "# Alternatively, you can provide azure machine learning deployment cluster name (environment name) and resource group name\n",
    "# to deploy your model. It will use the provided cluster to deploy. To do that, please uncomment the following lines to create \n",
    "# the deployment object.\n",
    "\n",
    "# azureml_rscgroup = \"<resource group>\"\n",
    "# cluster_name = \"<cluster name>\"\n",
    "# deploy_obj = AMLDeployment(deployment_name=deployment_name, associated_DNNModel=my_detector,\n",
    "#                            aml_env=\"cluster\", cluster_name=cluster_name, resource_group=azureml_rscgroup, replicas=1)\n",
    "\n",
    "# Check if the deployment name exists, if yes remove it first.\n",
    "if deploy_obj.is_existing_service():\n",
    "    AMLDeployment.delete_if_service_exist(deployment_name)\n",
    "    \n",
    "# create the webservice\n",
    "print(\"Deploying to Azure cluster...\")\n",
    "deploy_obj.deploy()\n",
    "print(\"Deployment DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume the web service\n",
    "\n",
    "Once you created the webservice, you can score images with the deployed webservice. You have several options:\n",
    "\n",
    "   - You can directly score the webservice with the deployment object with: deploy_obj.score_image(image_path_or_url) \n",
    "   - Or, you can use the Service endpoint url and Service key (None for local deployment) with: AMLDeployment.score_existing_service_with_image(image_path_or_url, service_endpoint_url, service_key=None)\n",
    "   - Form your http requests directly to score the webservice endpoint (For advanced users)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score with existing deployment object\n",
    "```\n",
    "deploy_obj.score_image(image_path_or_url)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with existing deployment object\n",
    "\n",
    "# Score local image with file path\n",
    "print(\"Score local image with file path\")\n",
    "image_path_or_url = data_train.images[0].storage_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json[:50])\n",
    "\n",
    "# Score image url and remove image resizing\n",
    "print(\"Score image url\")\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url)\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time image scoring\n",
    "import timeit\n",
    "\n",
    "num_images = 3\n",
    "for img_index, img_obj in enumerate(data_train.images[:num_images]):\n",
    "    print(\"Calling API for image {} of {}: {}...\".format(img_index, num_images, img_obj.name))\n",
    "    tic = timeit.default_timer()\n",
    "    return_json = deploy_obj.score_image(img_obj.storage_path, image_resize_dims=[224,224])\n",
    "    print(\"   Time for API call: {:.2f} seconds\".format(timeit.default_timer() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score with service endpoint url and service key\n",
    "```\n",
    "    AMLDeployment.score_existing_service_with_image(image_path_or_url, service_endpoint_url, service_key=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related classes and functions\n",
    "from cvtk.operationalization import AMLDeployment\n",
    "\n",
    "service_endpoint_url = \"http://xxx\" # please replace with your own service url\n",
    "service_key = \"xxx\" # please replace with your own service key\n",
    "\n",
    "# score image url\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = AMLDeployment.score_existing_service_with_image(image_path_or_url,service_endpoint_url, service_key = service_key, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score endpoint with http request directly\n",
    "Following is some example code to form the http request directly in Python. You can do it in other programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_image_with_http(image, service_endpoint_url, service_key=None, parameters={}):\n",
    "    \"\"\"Score local image with http request\n",
    "\n",
    "    Args:\n",
    "        image (str): Image file path\n",
    "        service_endpoint_url(str): web service endpoint url\n",
    "        service_key(str): Service key. None for local deployment.\n",
    "        parameters (dict): Additional request paramters in dictionary. Default is {}.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        str: serialized result \n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    import json\n",
    "\n",
    "    if service_key is None:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "    else:\n",
    "        headers = {'Content-Type': 'application/json',\n",
    "                   \"Authorization\": ('Bearer ' + service_key)}\n",
    "    payload = []\n",
    "    encoded = None\n",
    "    \n",
    "    # Read image\n",
    "    with open(image,'rb') as f:\n",
    "        image_buffer = BytesIO(f.read()) ## Getting an image file represented as a BytesIO object\n",
    "        \n",
    "    # Convert your image to base64 string\n",
    "    # image_in_base64 : \"b'{base64}'\"\n",
    "    encoded = base64.b64encode(image_buffer.getvalue())\n",
    "    image_request = {\"image_in_base64\": \"{0}\".format(encoded), \"parameters\": parameters}\n",
    "    payload.append(image_request)\n",
    "    body = json.dumps(payload)\n",
    "    r = requests.post(service_endpoint_url, data=body, headers=headers)\n",
    "    try:\n",
    "        result = json.loads(r.text)\n",
    "        json.loads(result[0])\n",
    "    except:\n",
    "        raise ValueError(\"Incorrect output format. Result cant not be parsed: \" + r.text)\n",
    "    return result[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse serialized result from webservice\n",
    "The result from the web service is in json string that can be parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_or_url = image_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url)\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse result from json string\n",
    "import numpy as np\n",
    "parsed_result = TFFasterRCNN.parse_serialized_result(serialized_result_in_json)\n",
    "print(\"Parsed result:\", parsed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = detection_utils.visualize(image_path, parsed_result)\n",
    "path_save = \"../../../cvtk_output/scored_images/scored_image_web.jpg\"\n",
    "path_save_dir = os.path.dirname(os.path.abspath(path_save))\n",
    "os.makedirs(path_save_dir, exist_ok=True)\n",
    "ax.get_figure().savefig(path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDIX \n",
    "### (A) Using a pre-trained model\n",
    "\n",
    "Sometimes you may want to use a pre-trained model out-of-the-box. For example, since TensorFlow's Faster R-CNN model has been trained on the [COCO dataset](http://mscoco.org), you can initialize a model object and use its 'score' function to score images.\n",
    "\n",
    "#### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_detector_pt = TFFasterRCNN(labels=None, name=\"pretrained\")\n",
    "frozen_model_path, label_map_path = my_detector_pt.init_pretrained(use_frozen=True)\n",
    "print(\"Frozen model written to path: \" + frozen_model_path)\n",
    "print(\"Labels written to path: \" + label_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score with using preloaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'score' function can be used to score images and the returned scores can be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_dict = my_detector_pt.score(image_path)\n",
    "path_save = \"../../../cvtk_output/scored_images/scored_image_pretrained.jpg\"\n",
    "image_size = (8, 12)\n",
    "ax = detection_utils.visualize(image_path, detections_dict, label_map_path, path_save=path_save,\n",
    "                              image_size=image_size)\n",
    "# ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score with using frozen graph\n",
    "\n",
    "Alternatively, you can score images using a utility function. To do that, load the frozen graph first. Run this once as the loaded graph can be reused. The frozen graph is a single file that's generated from a graph definition and a set of checkpoints, stripping away all the nodes that aren't used for forward inference. For more information, see [A Tool Developer's Guide to TensorFlow Model Files](https://www.tensorflow.org/extend/tool_developers/). So a frozen graph is easier to maintain. However, frozen graph has been [reported](https://github.com/tensorflow/models/issues/3270) as being slow due to its lack of ability to optimize the GPU/CPU assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = detection_utils.load_graph(frozen_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then score an image and visualize the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_dict = detection_utils.score(detection_graph, image_path)\n",
    "path_save = \"../../../cvtk_output/scored_images/scored_image_pretrained_frozen.jpg\"\n",
    "image_size = (8, 12)\n",
    "ax = detection_utils.visualize(image_path, detections_dict, label_map_path, path_save=path_save,\n",
    "                              image_size=image_size)\n",
    "# ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Webcam scoring\n",
    "\n",
    "The model can also be used to read frames from a webcam (or optionally from disk) and score them. As detector, a pre-trained COCO model is used, but any trained detector can be used. \n",
    "\n",
    "To run the following code successfully using data from disk, it is recommended that you copy the code into a Python script, remove the line \"%matplotlib inline\" and run it outside of Jupyter notebook. Otherwise, only a single scored image shows up and the Jupyter kernel might stop responding. This problem does not occur if the images come from an actual webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvtk.core import Context, ObjectDetectionDataset, TFFasterRCNN\n",
    "from cvtk.utils.detection_utils import FilepathImageProvider, VideoImageProvider\n",
    "%matplotlib inline\n",
    "\n",
    "out_root_path = \"../../../cvtk_output\"\n",
    "Context.create(outputs_path=out_root_path, persistent_path=out_root_path, temp_path=out_root_path)\n",
    "\n",
    "# Initialize detector with pre-trained model\n",
    "my_detector = TFFasterRCNN(labels=None, name=\"pretrained\")\n",
    "my_detector.init_pretrained()\n",
    "\n",
    "# Choose image provider\n",
    "image_provider = VideoImageProvider() # read images from webcam\n",
    "# image_folder = \"../sample_data/foods/test\"\n",
    "# data_val = ObjectDetectionDataset.create_from_dir(dataset_name='val_dataset', data_dir=image_folder)\n",
    "# image_provider = FilepathImageProvider([image.storage_path for image in data_val.images])  #read images from disk\n",
    "# image_provider = VideoImageProvider(cv2_video_capture=cv2.VideoCapture(\"movie.mp4\")) #read images from video file\n",
    "\n",
    "# Run object detection\n",
    "_ = my_detector.score_multiple(image_provider, visualize=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© 2018 Microsoft. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
