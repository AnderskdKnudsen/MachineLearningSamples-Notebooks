{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating LSTM model with Azure Machine Learning Package for Forecasting \n",
    "\n",
    "In this notebook, learn how to integrate LSTM model in the framework provided by Azure Machine Learning Package for Forecasting (AMLPF) to quickly build and deploy a forecasting model. \n",
    "We will use dow jones dataset to build a model that forecasts quarterly revenue for these 30 dow jones listed companies.\n",
    "\n",
    "#### Disclaimer: \n",
    "This notebook is based on the ongoing development work as part of the future release of AMLPF. Therefore, please consider this as a preview of what might become available in future as part of AMLPF. \n",
    "Further, please note that this work has currently been tested only on Windows platform.\n",
    "\n",
    "### Prerequisites:\n",
    "If you don't have an Azure subscription, create a free account before you begin. The following accounts and application must be set up and installed:<br/>\n",
    "* Azure Machine Learning Experimentation account.\n",
    "* Azure Machine Learning Model Management account.\n",
    "* Azure Machine Learning Workbench installed.\n",
    "\n",
    "If these three are not yet created or installed, follow the Azure Machine Learning Quickstart and Workbench installation article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # comment out this statement if you do not want to suppress the warnings.\n",
    "\n",
    "import sys, os, inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import ftk\n",
    "ftk_root_path = (ftk.__path__)[0] # This is the path where ftk package is installed.\n",
    "\n",
    "from ftk.pipeline import AzureMLForecastPipeline\n",
    "from ftk.operationalization.forecast_webservice_factory import ForecastWebserviceFactory\n",
    "from ftk.operationalization.forecast_web_service import ForecastWebService\n",
    "from azuremltkbase.deployment.aml_settings import AMLSettings\n",
    "\n",
    "from ftk.operationalization.dnnscorecontext import DnnScoreContext\n",
    "from ftk.dnn_utils import create_lag_lead_features\n",
    "from ftk.dnn_utils import pickle_keras_models\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import load_model\n",
    "\n",
    "print('imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000) # Set random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = ftk_root_path + \"\\\\data\\\\dow_jones\\\\dow_jones_data.tsv\" # Change it depending upon where this file is stored.\n",
    "num_lag_feats = 16 # Number of lag features to be used while training the model.\n",
    "num_leads = 0 # Lead zero indicates current-time's value. forecast only one step at a time. \n",
    "# Note: MAPE error computation is done considering num_leads = 0. It may need to be updated to take into account num_leads > 0. It has not been done yet.\n",
    "num_test_records = 4 # Keep last four records for each company in the test data.\n",
    "num_lstm_au = 50 # Number of units in single lstm layer.\n",
    "num_epochs = 150 # Number of epochs to fit the model. \n",
    "dj_series_freq = 'Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  quarter_start company_ticker  revenue\n",
      "0    2000-01-01           AAPL  1945.00\n",
      "1    2000-04-01           AAPL  1825.00\n",
      "2    2000-07-01           AAPL  1870.00\n",
      "3    2000-10-01           AAPL  1007.00\n",
      "4    2001-01-01           AAPL  1431.00\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1860 entries, 0 to 1859\n",
      "Data columns (total 3 columns):\n",
      "quarter_start     1860 non-null object\n",
      "company_ticker    1860 non-null object\n",
      "revenue           1835 non-null float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 43.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read the dow_jones_data.\n",
    "dj_df = pd.read_table(data_file_path)\n",
    "print(dj_df.head())\n",
    "print(dj_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1798 entries, 0 to 1859\n",
      "Data columns (total 3 columns):\n",
      "quarter_start     1798 non-null datetime64[ns]\n",
      "company_ticker    1798 non-null object\n",
      "revenue           1798 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 56.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Revenue has null values for some company. 'V' has been such identified company.\n",
    "# In this experiment, we remove the company from the dataset instead of interpolating.\n",
    "dj_df = dj_df[dj_df['company_ticker'] != 'V'] \n",
    "# Convert quarter_start field to datetime.\n",
    "dj_df['quarter_start'] = pd.to_datetime(dj_df['quarter_start'])\n",
    "print(dj_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by company to normalize it accordingly.\n",
    "grouped_data = dj_df.groupby(by='company_ticker')\n",
    "cmp_to_scaler = {}\n",
    "norm_dj_df = pd.DataFrame(columns=dj_df.columns) # Dataframe with quarter_start, company_ticker, normalized-revenue information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each company's data individually and save the scaler into a dictionary to be used later.\n",
    "for grp_name, grp_data in grouped_data:\n",
    "    cur_grp_data = grp_data.sort_values(by=['quarter_start'])\n",
    "    cur_grp_data = cur_grp_data.drop(['company_ticker', 'quarter_start'], axis=1)\n",
    "    scaler = MinMaxScaler(feature_range=(0.000001, 1)) \n",
    "    norm_grp_data = scaler.fit_transform(cur_grp_data)    \n",
    "    cmp_to_scaler[grp_name] = scaler\n",
    "    norm_grp_df = pd.DataFrame(norm_grp_data, columns=['revenue'])\n",
    "    aux_data_df = grp_data.loc[:,('quarter_start', 'company_ticker')]\n",
    "    aux_data_df.reset_index(drop=True, inplace=True)\n",
    "    cur_grp_norm_df = pd.concat((aux_data_df, norm_grp_df), axis=1)\n",
    "    norm_dj_df = norm_dj_df.append(cur_grp_norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 16 lags as features for each quarterly data point (normalized revenue in previous step).\n",
    "dj_reg = pd.DataFrame()\n",
    "norm_grp_data = norm_dj_df.groupby(by='company_ticker')\n",
    "for grp_name, grp_data in norm_grp_data:\n",
    "    cur_grp_data = grp_data.sort_values(by=['quarter_start'])\n",
    "    dj_reg_grp = create_lag_lead_features(cur_grp_data, ts_col='revenue', \n",
    "                    aux_cols=['company_ticker', 'quarter_start'], num_lags=num_lag_feats)\n",
    "    dj_reg = dj_reg.append(dj_reg_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of feature column-names.\n",
    "feat_cols = []\n",
    "feat_tgt_cols = []\n",
    "for i in range(num_lag_feats, 0, -1) :\n",
    "    feat_cols.append('revenueLag' + str(i))\n",
    "feat_tgt_cols.extend(feat_cols)\n",
    "\n",
    "# Create list of target column-names. \n",
    "target_cols = ['revenueLead0']\n",
    "for i in range(1, num_leads+1) :\n",
    "    target_cols.append('revenueLead' + str(i))\n",
    "feat_tgt_cols.extend(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into taining and test dataset for each company.\n",
    "dj_reg_grp_data = dj_reg.groupby(by='company_ticker')\n",
    "train_data = pd.DataFrame(columns=dj_reg.columns)\n",
    "test_data = pd.DataFrame(columns=dj_reg.columns)\n",
    "\n",
    "for grp_name, grp_data in dj_reg_grp_data:\n",
    "    cur_grp_data = grp_data.sort_values(by=['quarter_start'])\n",
    "    num_records = cur_grp_data.shape[0]\n",
    "    train_data = train_data.append(pd.DataFrame(cur_grp_data.iloc[:(num_records - num_test_records),:]))\n",
    "    test_data = test_data.append(pd.DataFrame(cur_grp_data.iloc[(num_records - num_test_records):,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1218, 1, 16)\n",
      "(1218, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract features and target values for training data.\n",
    "train_X = train_data[feat_cols] \n",
    "train_Y = train_data[target_cols]\n",
    "\"\"\"\n",
    "Formatting the input to be of the shape (number_of_samples, timesteps, number_of_features). \n",
    "For detail explanation refer to https://keras.io/layers/recurrent/.\n",
    "Note: I am considering here single timestep (set to 1) and number of features to be 16. It could be specified in \n",
    "a different way (I mean, 16 timesteps instead of 1) and I plan to experiment that in future.\n",
    "\"\"\"\n",
    "train_X = train_X.values.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "train_Y = train_Y.values.reshape((train_Y.shape[0], train_Y.shape[1]))\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LSTM network.\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_lstm_au, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1)) #dimension of the output vector \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " - 2s - loss: 0.0180\n",
      "Epoch 2/150\n",
      " - 2s - loss: 0.0131\n",
      "Epoch 3/150\n",
      " - 2s - loss: 0.0119\n",
      "Epoch 4/150\n",
      " - 2s - loss: 0.0114\n",
      "Epoch 5/150\n",
      " - 3s - loss: 0.0111\n",
      "Epoch 6/150\n",
      " - 2s - loss: 0.0108\n",
      "Epoch 7/150\n",
      " - 3s - loss: 0.0106\n",
      "Epoch 8/150\n",
      " - 2s - loss: 0.0105\n",
      "Epoch 9/150\n",
      " - 2s - loss: 0.0104\n",
      "Epoch 10/150\n",
      " - 2s - loss: 0.0102\n",
      "Epoch 11/150\n",
      " - 2s - loss: 0.0102\n",
      "Epoch 12/150\n",
      " - 2s - loss: 0.0101\n",
      "Epoch 13/150\n",
      " - 2s - loss: 0.0100\n",
      "Epoch 14/150\n",
      " - 2s - loss: 0.0099\n",
      "Epoch 15/150\n",
      " - 2s - loss: 0.0099\n",
      "Epoch 16/150\n",
      " - 2s - loss: 0.0098\n",
      "Epoch 17/150\n",
      " - 2s - loss: 0.0097\n",
      "Epoch 18/150\n",
      " - 2s - loss: 0.0097\n",
      "Epoch 19/150\n",
      " - 2s - loss: 0.0096\n",
      "Epoch 20/150\n",
      " - 2s - loss: 0.0096\n",
      "Epoch 21/150\n",
      " - 2s - loss: 0.0095\n",
      "Epoch 22/150\n",
      " - 2s - loss: 0.0095\n",
      "Epoch 23/150\n",
      " - 2s - loss: 0.0094\n",
      "Epoch 24/150\n",
      " - 2s - loss: 0.0094\n",
      "Epoch 25/150\n",
      " - 2s - loss: 0.0093\n",
      "Epoch 26/150\n",
      " - 2s - loss: 0.0093\n",
      "Epoch 27/150\n",
      " - 2s - loss: 0.0093\n",
      "Epoch 28/150\n",
      " - 2s - loss: 0.0092\n",
      "Epoch 29/150\n",
      " - 2s - loss: 0.0092\n",
      "Epoch 30/150\n",
      " - 2s - loss: 0.0091\n",
      "Epoch 31/150\n",
      " - 2s - loss: 0.0091\n",
      "Epoch 32/150\n",
      " - 2s - loss: 0.0091\n",
      "Epoch 33/150\n",
      " - 2s - loss: 0.0090\n",
      "Epoch 34/150\n",
      " - 3s - loss: 0.0090\n",
      "Epoch 35/150\n",
      " - 2s - loss: 0.0090\n",
      "Epoch 36/150\n",
      " - 2s - loss: 0.0089\n",
      "Epoch 37/150\n",
      " - 2s - loss: 0.0089\n",
      "Epoch 38/150\n",
      " - 2s - loss: 0.0089\n",
      "Epoch 39/150\n",
      " - 2s - loss: 0.0088\n",
      "Epoch 40/150\n",
      " - 2s - loss: 0.0088\n",
      "Epoch 41/150\n",
      " - 2s - loss: 0.0088\n",
      "Epoch 42/150\n",
      " - 2s - loss: 0.0087\n",
      "Epoch 43/150\n",
      " - 2s - loss: 0.0087\n",
      "Epoch 44/150\n",
      " - 2s - loss: 0.0087\n",
      "Epoch 45/150\n",
      " - 2s - loss: 0.0087\n",
      "Epoch 46/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 47/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 48/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 49/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 50/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 51/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 52/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 53/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 54/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 55/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 56/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 57/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 58/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 59/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 60/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 61/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 62/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 63/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 64/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 65/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 66/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 67/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 68/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 69/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 70/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 71/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 72/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 73/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 74/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 75/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 76/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 77/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 78/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 79/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 80/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 81/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 82/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 83/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 84/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 85/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 86/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 87/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 88/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 89/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 90/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 91/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 92/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 93/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 94/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 95/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 96/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 97/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 98/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 99/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 100/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 101/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 102/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 103/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 104/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 105/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 106/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 107/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 108/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 109/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 110/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 111/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 112/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 113/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 114/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 115/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 116/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 117/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 118/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 119/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 120/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 121/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 122/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 123/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 124/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 125/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 126/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 127/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 128/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 129/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 130/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 131/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 132/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 133/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 134/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 135/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 136/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 137/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 138/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 139/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 140/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 141/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 142/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 143/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 144/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 145/150\n",
      " - 2s - loss: 0.0068\n",
      "Epoch 146/150\n",
      " - 2s - loss: 0.0068\n",
      "Epoch 147/150\n",
      " - 2s - loss: 0.0068\n",
      "Epoch 148/150\n",
      " - 3s - loss: 0.0068\n",
      "Epoch 149/150\n",
      " - 2s - loss: 0.0068\n",
      "Epoch 150/150\n",
      " - 3s - loss: 0.0068\n"
     ]
    }
   ],
   "source": [
    "# Fit network. Currently set the batch_size=1; will add more relevant information on this later.\n",
    "history = model.fit(train_X, train_Y, epochs=num_epochs, batch_size=1, verbose=2, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50)                13400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,451\n",
      "Trainable params: 13,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print model.summary.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_keras_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_subscription = '<subscription name>'\n",
    "\n",
    "# Two deployment modes are supported: 'local' and 'cluster'. \n",
    "# 'local' deployment deploys to a local docker container.\n",
    "# 'cluster' deployment deploys to a Azure Container Service Kubernetes-based cluster\n",
    "cluster = '<deployment mode>'\n",
    "\n",
    "# The AML environment name. This could be an existing AML environment or a new AML environment to be created automatically.\n",
    "aml_env_name = '<deployment env name>'\n",
    "\n",
    "# The resource group that contains the Azure resources related to the AML environment.\n",
    "aml_env_resource_group = '<env resource group name>'\n",
    "\n",
    "# The location where the Azure resources related to the AML environment are located at.\n",
    "aml_env_location = '<env resource location>'\n",
    "\n",
    "# The AML model management account name. This could be an existing model management account a new model management \n",
    "# account to be created automatically. \n",
    "model_management_account_name = '<model management account name>'\n",
    "\n",
    "# The resource group that contains the Azure resources related to the model management account.\n",
    "# Could set set to aml_env_resource_group.\n",
    "model_management_account_resource_group = '<model management account resource group>' \n",
    "\n",
    "\n",
    "# The location where the Azure resources related to the model management account are located at.\n",
    "# Could be set to aml_env_location.\n",
    "model_management_account_location = '<model management account location>'\n",
    "\n",
    "\n",
    "# The name of the deployment/web service.\n",
    "deployment_name =  '<web service name>'\n",
    "\n",
    "# The directory to store deployment related files, such as pipeline pickle file, score script, and conda dependencies file. \n",
    "deployment_working_directory = '<local working directory>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_settings = AMLSettings(env_name=aml_env_name,\n",
    "                         env_resource_group=aml_env_resource_group,\n",
    "                         env_location=aml_env_location, \n",
    "                         azure_subscription=azure_subscription,\n",
    "                         model_management_account_name=model_management_account_name, \n",
    "                         model_management_account_resource_group=model_management_account_resource_group,\n",
    "                         model_management_account_location=model_management_account_location,\n",
    "                         cluster=cluster)\n",
    "\n",
    "pipeline_lstm = AzureMLForecastPipeline([('lstm_model', model)])\n",
    "aml_deployment = ForecastWebserviceFactory(aml_settings=aml_settings, \n",
    "                               deployment_name=deployment_name,\n",
    "                               pipeline=pipeline_lstm, \n",
    "                               deployment_working_directory=deployment_working_directory)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step can take 5 to 20 minutes\n",
    "# NOTE: Currently, recreate=False is not supported for deploying to ACS clusters\n",
    "aml_deployment.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = ForecastWebService(deployment_name) # Get a refernce to the deployed web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 2018-05-04 18:30:56,598 INFO Web service scored. \n",
      "Company: AAPL Test MAPE: 7.855\n",
      "F1 2018-05-04 18:30:56,929 INFO Web service scored. \n",
      "Company: AXP Test MAPE: 4.809\n",
      "F1 2018-05-04 18:30:57,251 INFO Web service scored. \n",
      "Company: BA Test MAPE: 6.605\n",
      "F1 2018-05-04 18:30:57,582 INFO Web service scored. \n",
      "Company: CAT Test MAPE: 3.890\n",
      "F1 2018-05-04 18:30:57,898 INFO Web service scored. \n",
      "Company: CSCO Test MAPE: 5.856\n",
      "F1 2018-05-04 18:30:58,230 INFO Web service scored. \n",
      "Company: CVX Test MAPE: 12.393\n",
      "F1 2018-05-04 18:30:58,553 INFO Web service scored. \n",
      "Company: DD Test MAPE: 2.779\n",
      "F1 2018-05-04 18:30:58,884 INFO Web service scored. \n",
      "Company: DIS Test MAPE: 4.269\n",
      "F1 2018-05-04 18:30:59,200 INFO Web service scored. \n",
      "Company: GE Test MAPE: 9.566\n",
      "F1 2018-05-04 18:30:59,517 INFO Web service scored. \n",
      "Company: GS Test MAPE: 9.196\n",
      "F1 2018-05-04 18:30:59,854 INFO Web service scored. \n",
      "Company: HD Test MAPE: 2.951\n",
      "F1 2018-05-04 18:31:00,170 INFO Web service scored. \n",
      "Company: IBM Test MAPE: 7.367\n",
      "F1 2018-05-04 18:31:00,502 INFO Web service scored. \n",
      "Company: INTC Test MAPE: 4.590\n",
      "F1 2018-05-04 18:31:00,818 INFO Web service scored. \n",
      "Company: JNJ Test MAPE: 0.825\n",
      "F1 2018-05-04 18:31:01,135 INFO Web service scored. \n",
      "Company: JPM Test MAPE: 3.166\n",
      "F1 2018-05-04 18:31:01,457 INFO Web service scored. \n",
      "Company: KO Test MAPE: 3.492\n",
      "F1 2018-05-04 18:31:01,789 INFO Web service scored. \n",
      "Company: MCD Test MAPE: 2.921\n",
      "F1 2018-05-04 18:31:02,124 INFO Web service scored. \n",
      "Company: MMM Test MAPE: 2.112\n",
      "F1 2018-05-04 18:31:02,436 INFO Web service scored. \n",
      "Company: MRK Test MAPE: 15.878\n",
      "F1 2018-05-04 18:31:02,759 INFO Web service scored. \n",
      "Company: MSFT Test MAPE: 3.398\n",
      "F1 2018-05-04 18:31:03,074 INFO Web service scored. \n",
      "Company: NKE Test MAPE: 5.586\n",
      "F1 2018-05-04 18:31:03,406 INFO Web service scored. \n",
      "Company: PFE Test MAPE: 4.166\n",
      "F1 2018-05-04 18:31:03,738 INFO Web service scored. \n",
      "Company: PG Test MAPE: 2.775\n",
      "F1 2018-05-04 18:31:04,058 INFO Web service scored. \n",
      "Company: TRV Test MAPE: 5.974\n",
      "F1 2018-05-04 18:31:04,376 INFO Web service scored. \n",
      "Company: UNH Test MAPE: 6.617\n",
      "F1 2018-05-04 18:31:04,707 INFO Web service scored. \n",
      "Company: UTX Test MAPE: 3.160\n",
      "F1 2018-05-04 18:31:05,023 INFO Web service scored. \n",
      "Company: VZ Test MAPE: 3.925\n",
      "F1 2018-05-04 18:31:05,361 INFO Web service scored. \n",
      "Company: WMT Test MAPE: 3.881\n",
      "F1 2018-05-04 18:31:05,693 INFO Web service scored. \n",
      "Company: XOM Test MAPE: 14.168\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataframe with column-names to hold forecasts and other relevant information.\n",
    "final_test_forecasts = pd.DataFrame(columns=['company_ticker', 'quarter_start', 'actual', 'forecast'])\n",
    "\n",
    "# Initialize dataframe with column-names to hold MAPE (Mean Absolute Percentage Error) for each company.\n",
    "final_mapes = pd.DataFrame(columns=['company_ticker', 'mape'])\n",
    "\n",
    "\"\"\"\n",
    "Compute prediction of test data one company at a time. \n",
    "This is to simplify the process of scaling it back to original scale for that company.\n",
    "\"\"\"\n",
    "test_grp_data = test_data.groupby(by='company_ticker')\n",
    "\n",
    "for grp_name, grp_data in test_grp_data:\n",
    "    cur_grp_data = grp_data.reset_index(drop=True)\n",
    "    cur_grp_data['quarter_start'] = pd.to_datetime(cur_grp_data['quarter_start'])\n",
    "    cur_grp_data = cur_grp_data.sort_values(by=['quarter_start'])\n",
    "    cur_final_test_fcasts = cur_grp_data[['company_ticker', 'quarter_start']]\n",
    "    scaler = cmp_to_scaler[grp_name]\n",
    "\n",
    "    test_X = cur_grp_data[feat_cols]\n",
    "    test_Y = cur_grp_data[target_cols]\n",
    "    test_X_reshape = test_X.values.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    \n",
    "    dnnscoreobject = DnnScoreContext(input_scoring_data=test_X_reshape, \n",
    "                             pipeline_execution_type='predict') # construct a context object to be used for scoring purpose.\n",
    "    yhat = service.score(score_context=dnnscoreobject) # invoke the web service to get predictions on the test data.\n",
    "    \n",
    "    inv_x_yhat = pd.concat((test_X, pd.DataFrame(yhat)), axis=1)      \n",
    "    inv_x_yhat = scaler.inverse_transform(inv_x_yhat)    \n",
    "    inv_x_yhat_df = pd.DataFrame(inv_x_yhat, columns=feat_tgt_cols)\n",
    "    inv_yhat = inv_x_yhat_df[target_cols] \n",
    "    cur_final_test_fcasts['forecast'] = inv_yhat\n",
    "        \n",
    "    inv_x_y = pd.concat((test_X, pd.DataFrame(test_Y)), axis=1)\n",
    "    inv_x_y = scaler.inverse_transform(inv_x_y)\n",
    "    inv_x_y_df = pd.DataFrame(inv_x_y, columns=feat_tgt_cols)\n",
    "    inv_y = inv_x_y_df[target_cols]\n",
    "    cur_final_test_fcasts['actual'] = inv_y\n",
    "\n",
    "    final_test_forecasts = final_test_forecasts.append(cur_final_test_fcasts)\n",
    "    mape = (np.mean(np.abs((inv_y - inv_yhat)/inv_y)))*100\n",
    "    print('Company: ' + grp_name + ' Test MAPE: %.3f' % mape)\n",
    "    final_mapes = final_mapes.append({'company_ticker' : grp_name, 'mape' : mape}, ignore_index=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
